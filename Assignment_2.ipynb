{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import package"
      ],
      "metadata": {
        "id": "qtXi8SB2oM89"
      },
      "id": "qtXi8SB2oM89"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e8c23cd",
      "metadata": {
        "id": "5e8c23cd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from sklearn.cluster import AffinityPropagation, DBSCAN, Birch\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
        "from scipy.spatial.distance import cdist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f964f93",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "6f964f93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "4ae399bc-70ce-402a-d0e3-d03c93ec8998"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'online_shoppers_intention.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c9b9a19e9d22>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Import data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mshoppers_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"online_shoppers_intention.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Normalize when applicable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'online_shoppers_intention.csv'"
          ]
        }
      ],
      "source": [
        "# Normalization function\n",
        "def normalize(dataColumn):\n",
        "    min_value = min(dataColumn)\n",
        "    max_value = max(dataColumn)\n",
        "\n",
        "    normalized_data = [(x-min_value)/(max_value - min_value) for x in dataColumn]\n",
        "\n",
        "    return normalized_data\n",
        "\n",
        "# Import data\n",
        "shoppers_data = pd.read_csv(\"online_shoppers_intention.csv\")\n",
        "\n",
        "# Normalize when applicable\n",
        "float_shoppers_data = shoppers_data.select_dtypes(\"float\")\n",
        "\n",
        "for i in float_shoppers_data.columns:\n",
        "    shoppers_data[i] = normalize(float_shoppers_data[i])\n",
        "\n",
        "int_shoppers_data = shoppers_data.select_dtypes(\"int\")\n",
        "\n",
        "for i in int_shoppers_data.columns:\n",
        "    shoppers_data[i] = normalize(int_shoppers_data[i])\n",
        "\n",
        "print(shoppers_data.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32f84ce6",
      "metadata": {
        "id": "32f84ce6"
      },
      "outputs": [],
      "source": [
        "# Taking the relevant values for machine learning\n",
        "columns_to_drop = ['Month', 'VisitorType']\n",
        "numerical_shoppers_data = shoppers_data.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "# Recoding boolean values\n",
        "numerical_shoppers_data['Weekend'] = numerical_shoppers_data['Weekend'].map({True: 1, False: 0})\n",
        "numerical_shoppers_data['Revenue'] = numerical_shoppers_data['Revenue'].map({True: 1, False: 0})\n",
        "\n",
        "print(numerical_shoppers_data.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1"
      ],
      "metadata": {
        "id": "NefjskAWpJqq"
      },
      "id": "NefjskAWpJqq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Comparison between browser 13 and other browsers"
      ],
      "metadata": {
        "id": "wyKk7CDDpM0g"
      },
      "id": "wyKk7CDDpM0g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e12e9d2",
      "metadata": {
        "id": "6e12e9d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "f50452c9-529d-4fd3-9dff-8f494f59b978"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'shoppers_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-30da1886e62e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb13_shoppers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshoppers_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Browser == 1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mother_shoppers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshoppers_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Browser != 1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m df = {\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"Administrative Mean\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb13_shoppers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Administrative\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_shoppers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Administrative\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'shoppers_data' is not defined"
          ]
        }
      ],
      "source": [
        "b13_shoppers = shoppers_data.query('Browser == 1')\n",
        "other_shoppers = shoppers_data.query('Browser != 1')\n",
        "\n",
        "df = {\n",
        "    \"Administrative Mean\": [b13_shoppers[\"Administrative\"].mean(), other_shoppers[\"Administrative\"].mean()],\n",
        "    \"Administrative_Duration Mean\": [b13_shoppers[\"Administrative_Duration\"].mean(), other_shoppers[\"Administrative_Duration\"].mean()],\n",
        "    \"Informational Mean\": [b13_shoppers[\"Informational\"].mean(), other_shoppers[\"Informational\"].mean()],\n",
        "    \"Informational_Duration Mean\": [b13_shoppers[\"Informational_Duration\"].mean(), other_shoppers[\"Informational_Duration\"].mean()],\n",
        "    \"ProductRelated Mean\": [b13_shoppers[\"ProductRelated\"].mean(), other_shoppers[\"ProductRelated\"].mean()],\n",
        "    \"ProductRelated_Duration Mean\": [b13_shoppers[\"ProductRelated_Duration\"].mean(), other_shoppers[\"ProductRelated_Duration\"].mean()],\n",
        "    \"BounceRates Mean\": [b13_shoppers[\"BounceRates\"].mean(), other_shoppers[\"BounceRates\"].mean()],\n",
        "    \"ExitRates Mean\": [b13_shoppers[\"ExitRates\"].mean(), other_shoppers[\"ExitRates\"].mean()],\n",
        "    \"PageValues Mean\": [b13_shoppers[\"PageValues\"].mean(), other_shoppers[\"PageValues\"].mean()],\n",
        "    \"SpecialDay Mean\": [b13_shoppers[\"SpecialDay\"].mean(), other_shoppers[\"SpecialDay\"].mean()],\n",
        "    # \"OperatingSystems Mean\": [b13_shoppers[\"OperatingSystems\"].mean(), other_shoppers[\"OperatingSystems\"].mean()],\n",
        "    # \"Region Mean\": [b13_shoppers[\"Region\"].mean(), other_shoppers[\"Region\"].mean()],\n",
        "    # \"TrafficType Mean\": [b13_shoppers[\"TrafficType\"].mean(), other_shoppers[\"TrafficType\"].mean()],\n",
        "    \"Weekend Mean\": [b13_shoppers[\"Weekend\"].mean(), other_shoppers[\"Weekend\"].mean()],\n",
        "    \"Revenue Mean\": [b13_shoppers[\"Revenue\"].mean(), other_shoppers[\"Revenue\"].mean()],\n",
        "    }\n",
        "\n",
        "comparison_shoppers = pd.DataFrame(data=df)\n",
        "comparison_shoppers_melted = comparison_shoppers.melt(var_name='Variable', value_name='Value', ignore_index=False)\n",
        "comparison_shoppers_melted = comparison_shoppers_melted.reset_index()\n",
        "\n",
        "comparison_fig = px.bar(\n",
        "    data_frame=comparison_shoppers_melted,\n",
        "    x=\"Variable\",\n",
        "    y=\"Value\",\n",
        "    color=\"index\",\n",
        "    barmode=\"group\",\n",
        "    title = \"Browser 13 compared to other Browsers in different characteristics\",\n",
        "    )\n",
        "\n",
        "comparison_fig.update_layout(\n",
        "    xaxis=dict(tickfont=dict(family='Arial Black', size=14)),\n",
        "    yaxis=dict(tickfont=dict(family='Arial Black', size=14)),\n",
        "    xaxis_title = \"Characteristics\",\n",
        "    yaxis_title = \"Mean Value\"\n",
        "    )\n",
        "\n",
        "comparison_fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08cca239",
      "metadata": {
        "id": "08cca239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "0636e132-f6de-40e7-fc77-07e24467b409"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'numerical_shoppers_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2e4887d8726a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Apply PCA on data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_shoppers_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Revenue'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_shoppers_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Revenue'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'numerical_shoppers_data' is not defined"
          ]
        }
      ],
      "source": [
        "# Apply PCA on data\n",
        "X = numerical_shoppers_data.drop(columns=['Revenue'])\n",
        "y = numerical_shoppers_data['Revenue']\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "pca_df = pd.DataFrame(data=X_pca, columns=[f'Principal Component {i}' for i in range(1, 3)])\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"Explained Variance Ratio:\", explained_variance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7c5719a",
      "metadata": {
        "id": "c7c5719a"
      },
      "outputs": [],
      "source": [
        "# Visualise explained variance\n",
        "plt.bar([f'PC {i}' for i in range(1, 3)],\n",
        "        explained_variance, color='lightblue')\n",
        "plt.title('Explained Variance by Principal Components')\n",
        "plt.xlabel('Principal Components')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.show()\n",
        "\n",
        "colors = ['red' if label == 0 else 'purple' for label in y]\n",
        "plt.scatter(x=pca_df['Principal Component 1'], y=pca_df['Principal Component 2'],\n",
        "            c=colors)\n",
        "plt.title('Principal Components 1 vs 2')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "\n",
        "Viewer = mpatches.Patch(color='red', label='Revenue=0 (Viewer)')\n",
        "Buyer = mpatches.Patch(color='purple', label='Revenue=1(Buyer)')\n",
        "\n",
        "plt.legend(handles=[Viewer, Buyer])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3"
      ],
      "metadata": {
        "id": "bS-u4b0SvYzL"
      },
      "id": "bS-u4b0SvYzL"
    },
    {
      "cell_type": "markdown",
      "id": "00e7dd1c",
      "metadata": {
        "id": "00e7dd1c"
      },
      "source": [
        "## 3.1 Affiniaty Propagation Clustering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data_scaled_df is preprocessed dataset\n",
        "clustering = AffinityPropagation(random_state=5).fit(pca_df)\n",
        "\n",
        "# Labels\n",
        "labels = clustering.labels_\n",
        "\n",
        "# visualzation\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(pca_df['Principal Component 1'], pca_df['Principal Component 2'], c=labels, cmap='viridis')\n",
        "plt.colorbar(scatter)\n",
        "plt.title(\"Affinity Propagation Clustering (PC1 vs PC2)\")\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Oarax9VArQfx"
      },
      "id": "Oarax9VArQfx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3d8872b6",
      "metadata": {
        "id": "3d8872b6"
      },
      "source": [
        "## 3.2 DBSCAN Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ab6b070",
      "metadata": {
        "id": "0ab6b070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "1e5d72e1-a3d3-44b6-f839-40d8e460a547"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pca_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-cacb05788039>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Principal Component 1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Principal Component 2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdbscan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels_dbscan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbscan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pca_df' is not defined"
          ]
        }
      ],
      "source": [
        "X = pca_df[['Principal Component 1', 'Principal Component 2']]\n",
        "dbscan = DBSCAN(eps=0.4, min_samples=2).fit(X)\n",
        "\n",
        "# Labels\n",
        "labels_dbscan = dbscan.labels_\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(X['Principal Component 1'], X['Principal Component 2'], c=labels_dbscan, cmap='viridis')\n",
        "# plt.colorbar(scatter)\n",
        "plt.title(\"DBSCAN Clustering (Principal Component 1 vs principal Component 2)\")\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79a90023",
      "metadata": {
        "id": "79a90023"
      },
      "source": [
        "## 3.3 Birch Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adbbb600",
      "metadata": {
        "id": "adbbb600"
      },
      "outputs": [],
      "source": [
        "birch = Birch(n_clusters=3, threshold=0.5)\n",
        "birch.fit(X)\n",
        "\n",
        "# Get cluster labels\n",
        "labels_birch = birch.labels_\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(X['Principal Component 1'], X['Principal Component 2'], c=labels_birch, cmap='viridis')\n",
        "\n",
        "plt.title(\"BIRCH Clustering (Principal Component 1 vs Principal Componenet 2)\")\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4"
      ],
      "metadata": {
        "id": "BcyzdUXbvjuu"
      },
      "id": "BcyzdUXbvjuu"
    },
    {
      "cell_type": "markdown",
      "id": "24be9f08",
      "metadata": {
        "id": "24be9f08"
      },
      "source": [
        "## 4.1 Silhouette Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f3c0754",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "4f3c0754"
      },
      "outputs": [],
      "source": [
        "def euclidean_distance(point1, point2):\n",
        "    return np.sqrt(np.sum((point1 - point2) ** 2))\n",
        "\n",
        "def silhouette_score(points, labels):\n",
        "    n = len(points)\n",
        "    unique_clusters = set(labels)\n",
        "\n",
        "    silhouette_scores = []\n",
        "\n",
        "    for i in range(n):\n",
        "        current_point = points[i]\n",
        "        current_cluster = labels[i]\n",
        "\n",
        "        # Intra-cluster distance: average distance to all other points in the same cluster\n",
        "        same_cluster_points = [points[j] for j in range(n) if labels[j] == current_cluster and j != i]\n",
        "        if same_cluster_points:\n",
        "            a_i = sum(euclidean_distance(current_point, p) for p in same_cluster_points) / len(same_cluster_points)\n",
        "        else:\n",
        "            a_i = 0  # When there's no other point in the same cluster\n",
        "\n",
        "        # Inter-cluster distance: minimum average distance to points in any other cluster\n",
        "        b_i = float('inf')\n",
        "        for other_cluster in unique_clusters:\n",
        "            if other_cluster == current_cluster:\n",
        "                continue\n",
        "            other_cluster_points = [points[j] for j in range(n) if labels[j] == other_cluster]\n",
        "            if other_cluster_points:\n",
        "                avg_distance = sum(euclidean_distance(current_point, p) for p in other_cluster_points) / len(other_cluster_points)\n",
        "                b_i = min(b_i, avg_distance)\n",
        "\n",
        "        # Calculate silhouette score for point i\n",
        "        if a_i == 0 and b_i == 0:\n",
        "            silhouette = 0\n",
        "        else:\n",
        "            silhouette = (b_i - a_i) / max(a_i, b_i)\n",
        "        silhouette_scores.append(silhouette)\n",
        "\n",
        "    # Average silhouette score for all points\n",
        "    overall_silhouette_score = sum(silhouette_scores) / n\n",
        "    return overall_silhouette_score\n",
        "\n",
        "print(f\"Silhouette Score for DBSCAN: {silhouette_score(X.values, labels_dbscan)}\")\n",
        "print(f\"Silhouette Score for BIRCH: {silhouette_score(X.values, labels_birch)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Davies-Bouldin Score"
      ],
      "metadata": {
        "id": "Q3eJUOqTtEVl"
      },
      "id": "Q3eJUOqTtEVl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3977a89b",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "3977a89b"
      },
      "outputs": [],
      "source": [
        "def evaluate_davies_bouldin(X, labels_dbscan, labels_birch):\n",
        "    score_dbscan = davies_bouldin_score(X, labels_dbscan)\n",
        "    score_birch = davies_bouldin_score(X, labels_birch)\n",
        "\n",
        "    print(f\"Davies-Bouldin Score for DBSCAN: {score_dbscan}\")\n",
        "    print(f\"Davies-Bouldin Score for BIRCH: {score_birch}\")\n",
        "    return [score_dbscan, score_birch]\n",
        "\n",
        "X = pca_df[['Principal Component 1', 'Principal Component 2']]\n",
        "evaluate_davies_bouldin(X, labels_dbscan, labels_birch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f410d32",
      "metadata": {
        "id": "8f410d32"
      },
      "source": [
        "##4.3 Calsinki-Harabasz Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98875a27",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "98875a27"
      },
      "outputs": [],
      "source": [
        "# Make a function to evaluate three models in Task 3 at once\n",
        "def evaluate_calinski_harabasz(X, labels_dbscan, labels_birch):\n",
        "    score_dbscan = calinski_harabasz_score(X, labels_dbscan)\n",
        "    score_birch = calinski_harabasz_score(X, labels_birch)\n",
        "\n",
        "    print(f\"Calinski-Harabasz Score for DBSCAN: {score_dbscan}\")\n",
        "    print(f\"Calinski-Harabasz Score for BIRCH: {score_birch}\")\n",
        "    return [score_dbscan, score_birch]\n",
        "\n",
        "## change the X to the clustering variable\n",
        "X = pca_df[['Principal Component 1', 'Principal Component 2']]\n",
        "evaluate_calinski_harabasz(X, labels_dbscan, labels_birch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison Chart for Evaluation"
      ],
      "metadata": {
        "id": "Aa_fU5r6tdtS"
      },
      "id": "Aa_fU5r6tdtS"
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_metrics = ['Silhouette Score']\n",
        "db_metrics = ['Davies-Bouldin Score']\n",
        "ch_metrics = ['Calinski Harabasz Index']\n",
        "scores = [\n",
        "    [silhouette_score(X.values, labels_dbscan), silhouette_score(X.values, labels_birch)],\n",
        "    [(evaluate_davies_bouldin(X, labels_dbscan, labels_birch))],\n",
        "    [(evaluate_calinski_harabasz(X, labels_dbscan, labels_birch))]\n",
        "]\n",
        "\n",
        "evaluation_fig_silhouette = go.Figure(data=[\n",
        "    go.Bar(name='DBSCAN', x=silhouette_metrics, y=[scores[0][0][0]]),\n",
        "    go.Bar(name='BIRCH', x=silhouette_metrics, y=[scores[0][0][1]])\n",
        "])\n",
        "\n",
        "evaluation_fig_silhouette.update_layout(\n",
        "    title='Comparison of Clustering Performance Metrics',\n",
        "    xaxis_title='Silhouette Score',\n",
        "    yaxis_title='Score',\n",
        "    barmode='group'\n",
        ")\n",
        "\n",
        "evaluation_fig_silhouette.show()\n",
        "\n",
        "evaluation_fig_db = go.Figure(data=[\n",
        "    go.Bar(name='DBSCAN', x=db_metrics, y=[scores[1][0][0]]),\n",
        "    go.Bar(name='BIRCH', x=db_metrics, y=[scores[1][0][1]])\n",
        "])\n",
        "\n",
        "evaluation_fig_db.update_layout(\n",
        "    title='Comparison of Clustering Performance Metrics',\n",
        "    xaxis_title='Davies Bouldin Score',\n",
        "    yaxis_title='Score',\n",
        "    barmode='group'\n",
        ")\n",
        "\n",
        "evaluation_fig_db.show()\n",
        "\n",
        "evaluation_fig_ch = go.Figure(data=[\n",
        "    go.Bar(name='DBSCAN', x=ch_metrics, y=[scores[2][0][0]]),\n",
        "    go.Bar(name='BIRCH', x=ch_metrics, y=[scores[2][0][1]])\n",
        "])\n",
        "\n",
        "evaluation_fig_ch.update_layout(\n",
        "    title='Comparison of Clustering Performance Metrics',\n",
        "    xaxis_title='Calsinki Harabasz Index',\n",
        "    yaxis_title='Score',\n",
        "    barmode='group'\n",
        ")\n",
        "\n",
        "evaluation_fig_ch.show()"
      ],
      "metadata": {
        "id": "hRZpBqUPtoUP"
      },
      "id": "hRZpBqUPtoUP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Distance Functions"
      ],
      "metadata": {
        "id": "PrwGB5vLuLFf"
      },
      "id": "PrwGB5vLuLFf"
    },
    {
      "cell_type": "markdown",
      "id": "9a6cbe16",
      "metadata": {
        "id": "9a6cbe16"
      },
      "source": [
        "## 5.1 Eucledian Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65aec394",
      "metadata": {
        "id": "65aec394"
      },
      "outputs": [],
      "source": [
        "X = pca_df[['Principal Component 1', 'Principal Component 2']]\n",
        "# euclidean_X = cdist (X, X, metric=euclidean_distance)\n",
        "\n",
        "dbscan = DBSCAN(eps=0.01, min_samples=2, metric=euclidean_distance).fit(X)\n",
        "# Labels\n",
        "euclidean_labels_dbscan = dbscan.labels_\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(X['Principal Component 1'], X['Principal Component 2'], c=euclidean_labels_dbscan, cmap='viridis')\n",
        "\n",
        "plt.title(\"DBSCAN Clustering with Euclidean Distance (Principal Component 1 vs principal Component 2)\")\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4aea41b",
      "metadata": {
        "id": "f4aea41b"
      },
      "source": [
        "## 5.2 Manhattan Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf319c3",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "caf319c3"
      },
      "outputs": [],
      "source": [
        "def manhattan_distance(point1, point2):\n",
        "    distance = 0\n",
        "    for x1, x2 in zip(point1, point2):\n",
        "        distance += abs(x1 - x2)\n",
        "    return distance\n",
        "\n",
        "X = pca_df[['Principal Component 1', 'Principal Component 2']]\n",
        "\n",
        "dbscan = DBSCAN(eps=0.01, min_samples=2, metric=manhattan_distance).fit(X)\n",
        "# Labels\n",
        "manhattan_labels_dbscan = dbscan.labels_\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(X['Principal Component 1'], X['Principal Component 2'], c=manhattan_labels_dbscan, cmap='viridis')\n",
        "plt.title(\"DBSCAN Clustering with Manhattan Distance (Principal Component 1 vs principal Component 2)\")\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Cosine Similarity\n",
        "\n"
      ],
      "metadata": {
        "id": "MhQeDqImud5X"
      },
      "id": "MhQeDqImud5X"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb27cb77",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "bb27cb77"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity_distance(point1, point2):\n",
        "    vec1 = np.array(point1)\n",
        "    vec2 = np.array(point2)\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    magnitude_vec1 = np.linalg.norm(vec1)\n",
        "    magnitude_vec2 = np.linalg.norm(vec2)\n",
        "    cosine_sim = dot_product / (magnitude_vec1 * magnitude_vec2)\n",
        "    return 1- cosine_sim\n",
        "\n",
        "X = pca_df[['Principal Component 1', 'Principal Component 2']]\n",
        "\n",
        "dbscan = DBSCAN(eps=0.01, min_samples=2, metric=cosine_similarity_distance).fit(X)\n",
        "# Labels\n",
        "cosine_labels_dbscan = dbscan.labels_\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(X['Principal Component 1'], X['Principal Component 2'], c=cosine_labels_dbscan, cmap='viridis')\n",
        "\n",
        "plt.title(\"DBSCAN Clustering with Cosine Similarity Distance (Principal Component 1 vs principal Component 2)\")\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dc540bc",
      "metadata": {
        "id": "9dc540bc"
      },
      "source": [
        "## 5.4 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84a0ab8f",
      "metadata": {
        "id": "84a0ab8f"
      },
      "outputs": [],
      "source": [
        "print(f\"Davies-Bouldin Score for DBSCAN with Eucledian Distance: {davies_bouldin_score(X, euclidean_labels_dbscan)}\")\n",
        "print(f\"Davies-Bouldin Score for DBSCAN with Manhattan Distance: {davies_bouldin_score(X, manhattan_labels_dbscan)}\")\n",
        "print(f\"Davies-Bouldin Score for DBSCAN with Cosine Distance: {davies_bouldin_score(X, cosine_labels_dbscan)}\")\n",
        "\n",
        "distances = ['Eucledian', 'Manhattan', 'Cosine Similarity']\n",
        "db_values = [davies_bouldin_score(X, euclidean_labels_dbscan), davies_bouldin_score(X, manhattan_labels_dbscan), davies_bouldin_score(X, cosine_labels_dbscan)]\n",
        "\n",
        "distance_comparison_fig = go.Figure(data=[\n",
        "    go.Bar(\n",
        "        x=distances,\n",
        "        y=db_values,\n",
        "        text=[f\"{db:.2f}\" for db in db_values],\n",
        "        textposition='auto'\n",
        "    )\n",
        "])\n",
        "\n",
        "distance_comparison_fig.update_layout(\n",
        "    title='Comparison of Davies-Bouldin Scores for Different Distance Metrics',\n",
        "    xaxis_title='Distance Metric',\n",
        "    yaxis_title='Davies-Bouldin Score',\n",
        "    yaxis=dict(range=[0, max(db_values) * 1.1])\n",
        ")\n",
        "\n",
        "distance_comparison_fig.show()"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}